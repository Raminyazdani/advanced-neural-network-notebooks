Ramin Yazdani | Advanced Neural Network Regularization Techniques | main | feat(notebook): Add neural network regularization notebook (dropout/weight-decay/early-stopping)

Implemented the second notebook demonstrating modern deep learning regularization techniques using PyTorch on MNIST:
- Dropout: Randomly deactivates neurons during training to prevent co-adaptation
- Weight Decay: L2 penalty applied directly in the optimizer
- Early Stopping: Halts training when validation performance plateaus

The notebook includes:
- MNIST dataset loading via torchvision
- Neural network architecture with dropout layers
- Training loop with weight decay and early stopping
- Visualization of training/validation curves
- Performance comparison of different regularization techniques

This complements the parameter norm penalties notebook, showing modern deep learning approaches.

Verified: neural_network_regularization.ipynb created with dropout, weight decay, and early stopping implementations.
